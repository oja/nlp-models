{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "word2vec.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "Or45t1_uxqhY",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext.datasets import PennTreebank"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LbygajhvxqiG",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "TEXT = data.Field(sequential=True, tokenize='spacy', lower=True)\n",
    "train, valid, test = PennTreebank.splits(TEXT)\n",
    "TEXT.build_vocab(train)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "context_size = 2\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
    "                                                            batch_size=512,\n",
    "                                                            bptt_len=context_size * 2 + 1, # context on both sides, plus the center word\n",
    "                                                            repeat=False)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "downloading ptb.train.txt\n",
      "downloading ptb.valid.txt\n",
      "downloading ptb.test.txt\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "\rptb.train.txt:   0%|          | 0.00/1.70M [00:00<?, ?B/s]",
      "\rptb.train.txt:   6%|▌         | 94.4k/1.70M [00:00<00:03, 474kB/s]",
      "\rptb.train.txt:   8%|▊         | 143k/1.70M [00:00<00:03, 408kB/s] ",
      "\rptb.train.txt:  11%|█▏        | 193k/1.70M [00:00<00:04, 356kB/s]",
      "\rptb.train.txt:  14%|█▍        | 242k/1.70M [00:00<00:04, 307kB/s]",
      "\rptb.train.txt:  20%|██        | 340k/1.70M [00:01<00:05, 255kB/s]",
      "\rptb.train.txt:  26%|██▌       | 442k/1.70M [00:01<00:04, 298kB/s]",
      "\rptb.train.txt:  32%|███▏      | 541k/1.70M [00:01<00:03, 313kB/s]",
      "\rptb.train.txt:  38%|███▊      | 640k/1.70M [00:01<00:02, 389kB/s]",
      "\rptb.train.txt:  49%|████▉     | 835k/1.70M [00:02<00:01, 487kB/s]",
      "\rptb.train.txt:  58%|█████▊    | 982k/1.70M [00:02<00:01, 545kB/s]",
      "\rptb.train.txt:  67%|██████▋   | 1.13M/1.70M [00:02<00:00, 644kB/s]",
      "\rptb.train.txt:  72%|███████▏  | 1.23M/1.70M [00:02<00:00, 567kB/s]",
      "\rptb.train.txt:  84%|████████▍ | 1.42M/1.70M [00:02<00:00, 701kB/s]",
      "\rptb.train.txt:  98%|█████████▊| 1.67M/1.70M [00:02<00:00, 866kB/s]",
      "\rptb.train.txt: 1.82MB [00:02, 965kB/s]                            ",
      "\rptb.train.txt: 2.01MB [00:03, 1.07MB/s]",
      "\rptb.train.txt: 2.26MB [00:03, 1.24MB/s]",
      "\rptb.train.txt: 2.46MB [00:03, 757kB/s] ",
      "\rptb.train.txt: 2.75MB [00:03, 948kB/s]",
      "\rptb.train.txt: 3.10MB [00:03, 1.20MB/s]",
      "\rptb.train.txt: 3.44MB [00:04, 1.44MB/s]",
      "\rptb.train.txt: 3.78MB [00:04, 1.75MB/s]",
      "\rptb.train.txt: 4.08MB [00:04, 1.64MB/s]",
      "\rptb.train.txt: 4.33MB [00:04, 1.57MB/s]",
      "\rptb.train.txt: 4.57MB [00:04, 1.28MB/s]",
      "\rptb.train.txt: 4.78MB [00:05, 1.16MB/s]",
      "\rptb.train.txt: 4.93MB [00:05, 969kB/s] ",
      "\rptb.train.txt: 5.10MB [00:05, 963kB/s]",
      "\n",
      "\rptb.valid.txt:   0%|          | 0.00/135k [00:00<?, ?B/s]",
      "\rptb.valid.txt: 288kB [00:00, 2.65MB/s]                   ",
      "\rptb.valid.txt: 400kB [00:00, 3.05MB/s]",
      "\n",
      "\rptb.test.txt:   0%|          | 0.00/150k [00:00<?, ?B/s]",
      "\rptb.test.txt:  65%|██████▍   | 97.1k/150k [00:00<00:00, 223kB/s]",
      "\rptb.test.txt: 196kB [00:00, 276kB/s]                            ",
      "\rptb.test.txt: 246kB [00:00, 268kB/s]",
      "\rptb.test.txt: 344kB [00:00, 308kB/s]",
      "\rptb.test.txt: 450kB [00:01, 428kB/s]",
      "\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bCgY8AV9-Sry",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = torch.sum(self.embeddings(inputs), dim=0)\n",
    "        out = self.linear(out)\n",
    "        out = F.log_softmax(out, dim=0)\n",
    "        return out"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m4iYnSgExqjZ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        out = self.linear(out)\n",
    "        out = F.log_softmax(out, dim=0)\n",
    "        return out"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "I1Sk0ReGAkKI",
    "colab": {}
   },
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, 100)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, target = batch.text, batch.text[context_size]\n",
    "        output = model(text)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}