{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGTmcjfJboig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import PennTreebank\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT01PYMYgnun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(sequential=True, tokenize='spacy', lower=True)\n",
        "train, valid, test = PennTreebank.splits(TEXT)\n",
        "TEXT.build_vocab(train, min_freq=5)\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "groups = 2 # train on pairs\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
        "                                                             batch_size=BATCH_SIZE,\n",
        "                                                             bptt_len=groups,\n",
        "                                                             repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTzEJJ9C0f1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GloVe(nn.Module):\n",
        "    def __init__(self, coo_matrix, embedding_dim):\n",
        "        super(GloVe, self).__init__()\n",
        "        \n",
        "        self.coo_matrix = coo_matrix\n",
        "        \n",
        "        # empirically found values from the paper\n",
        "        self.alpha = 0.75\n",
        "        self.cutoff = 100\n",
        "        \n",
        "        # laplacian smoothing\n",
        "        self.coo_matrix += 1\n",
        "        \n",
        "        self.vocab_size = coo_matrix.shape[0]\n",
        "        self.embedding_i = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bias_i = nn.Embedding(vocab_size, 1)\n",
        "        \n",
        "        self.embedding_j = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bias_j = nn.Embedding(vocab_size, 1)\n",
        "\n",
        "    def forward(self, word_i, word_j):\n",
        "        # word_i: [batch_size], long\n",
        "        # word_j: [batch_size], long\n",
        "        coos = Variable(torch.from_numpy(np.array([self.coo_matrix[word_i[x], word_j[x]] for x in range(BATCH_SIZE)]))).to(torch.long) # [batch_size]\n",
        "        weighting = Variable(torch.from_numpy(np.array([self._get_weighting(x) for x in coos]))).to(torch.long) # [batch_size]\n",
        "\n",
        "        embed_i_out = self.embedding_i(word_i)\n",
        "        bias_i_out = self.bias_i(word_i)\n",
        "        embed_j_out = self.embedding_j(word_j)\n",
        "        bias_j_out = self.bias_j(word_j)\n",
        "\n",
        "        return embed_i_out, embed_j_out, bias_i_out, bias_j_out, coos, weighting\n",
        "    \n",
        "    def _get_weighting(self, occur):\n",
        "        return 1.0 if occur > self.cutoff else (occur / self.cutoff) ** self.alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWxq8gfCFOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GloVeLoss(embed_i, embed_j, bias_i, bias_j, coos, weighting): # a modified version of MSELoss\n",
        "    return (torch.pow(((embed_i * embed_j).sum(1) + bias_i + bias_j) - torch.log(coos.to(torch.float)), 2) * weighting).sum()\n",
        "\n",
        "coo_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "\n",
        "model = GloVe(coo_matrix, 100)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1):\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        optimizer.zero_grad()\n",
        "        # batch: [groups, batch_size]\n",
        "        \n",
        "        text, target = batch.text[0], batch.target[-1] # not super proud of this, but whatever\n",
        "        embed_i, embed_j, bias_i, bias_j, coos, weighting = model(text, target)\n",
        "        loss = GloVeLoss(embed_i, embed_j, bias_i, bias_j, coos, weighting.to(torch.float))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}