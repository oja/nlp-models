{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.datasets import Multi30k\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "spacy_german = spacy.load('de')\n",
    "\n",
    "def tokenize_german(text):\n",
    "    return [tok.text for tok in spacy_german.tokenizer(text)][::-1] # reverse input\n",
    "\n",
    "SOURCE = data.Field(tokenize=tokenize_german,\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>', \n",
    "                    lower=True)\n",
    "\n",
    "TARGET = data.Field(tokenize='spacy',\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>',\n",
    "                    lower=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                    fields=(SOURCE, TARGET))\n",
    "    \n",
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                           batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.embedding(inputs)\n",
    "        out, (hidden, cell) = self.lstm(out)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 2)\n",
    "        self.prediction_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden, cell):\n",
    "        out = inputs.unsqueeze(0)\n",
    "        out = self.embedding(out)\n",
    "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
    "        prediction = self.prediction_layer(out.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        batch_size = target.shape[1]\n",
    "        max_length = target.shape[0]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(max_length, batch_size, target_vocab_size)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        inputs = target[0,:]\n",
    "        \n",
    "        for t in range(1, max_length):\n",
    "            output, hidden, cell = self.decoder(inputs, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < 0.5\n",
    "            top1 = output.max(1)[1]\n",
    "            inputs = (target[t] if teacher_force else top1)\n",
    "            \n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(18660, 100)\n    (lstm): LSTM(100, 200, num_layers=2)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(9799, 100)\n    (lstm): LSTM(100, 200, num_layers=2)\n    (prediction_layer): Linear(in_features=200, out_features=9799, bias=True)\n  )\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 24
    }
   ],
   "source": [
    "encoder = Encoder(len(SOURCE.vocab), 100, 200)\n",
    "decoder = Decoder(len(TARGET.vocab), 100, 200)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "def init_weights(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        nn.init.uniform_(parameter.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi['<pad>'])\n",
    "\n",
    "model.train()\n",
    "epoch_loss = 0\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    source = batch.src\n",
    "    target = batch.trg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(source, target)\n",
    "    output = output[1:].view(-1, output.shape[-1])\n",
    "    target = target[1:].view(-1)\n",
    "    \n",
    "    loss = loss_function(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "print(epoch_loss / len(train_iterator))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thanks to https://github.com/bentrevett/pytorch-seq2seq for reference\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}