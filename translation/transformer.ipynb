{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmrx6e7gIwLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext.datasets import Multi30k\n",
        "import spacy\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndMeT7mpLGB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_german = spacy.load('de')\n",
        "\n",
        "def tokenize_german(text):\n",
        "    return [tok.text for tok in spacy_german.tokenizer(text)][::-1] # reverse input\n",
        "\n",
        "SOURCE = data.Field(tokenize=tokenize_german,\n",
        "                    init_token=\"<sos>\",\n",
        "                    eos_token=\"<eos>\",\n",
        "                    lower=True)\n",
        "\n",
        "TARGET = data.Field(tokenize='spacy',\n",
        "                    init_token=\"<sos>\",\n",
        "                    eos_token=\"<eos>\",\n",
        "                    lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJwCPzXoN0ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, valid, test = Multi30k.splits(exts=('.de', '.en'),\n",
        "                                     fields=(SOURCE, TARGET))\n",
        "\n",
        "SOURCE.build_vocab(train, min_freq=3)\n",
        "TARGET.build_vocab(train, min_freq=3)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test),\n",
        "                                                                           batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUlSiEEu9Zf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_embed, target_embed, fully_connected):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.source_embed = source_embed\n",
        "        self.target_embed = target_embed\n",
        "        self.fully_connected = fully_connected\n",
        "        \n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)\n",
        "        \n",
        "    def encode(self, source, source_mask):\n",
        "        return self.encoder(source, source_mask)\n",
        "    \n",
        "    def decode(self, memory, source_mask, target, target_mask):\n",
        "        return self.decoder(self.target_embed(target), memory, source_mask, target_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXb0ayrz6PYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Paramater(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        mean = inputs.mean(-1, keepdim=True)\n",
        "        std = inputs.std(-1, keepdim=True)\n",
        "        return self.a_2 * (inputs - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCZ6bAhm641N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sublayer(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super(Sublayer, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, inputs, sublayer):\n",
        "        return inputs + self.dropout(sublayer(self.norm(inputs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jxTQDnFNHb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, depth):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = copies(layer, depth)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "    \n",
        "    def forward(self, inputs, mask):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer(inputs, mask)\n",
        "        return self.norm(inputs)    \n",
        "        \n",
        "    @staticmethod\n",
        "    def copies(module, count):\n",
        "        return nn.ModuleList([copy.deepcopy(module) for _ in range(count)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EygaSo2AO46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, attention, feedforward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayers = copies(Sublayer(size, dropout), 2)\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, inputs, mask):\n",
        "        x = self.sublayers[0](inputs, lambda inputs: self.attention(inputs, inputs, inputs, mask))\n",
        "        return self.sublayers[1](x, self.feed_forward)\n",
        "\n",
        "    @staticmethod\n",
        "    def copies(module, count):\n",
        "        return nn.ModuleList([copy.deepcopy(module) for _ in range(count)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7kUaLuNbeOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, depth):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = copies(layer, depth)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, inputs, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer(inputs, memory, src_mask, tgt_mask)\n",
        "        return self.norm(inputs)\n",
        "    \n",
        "    @staticmethod\n",
        "    def copies(module, count):\n",
        "        return nn.ModuleList([copy.deepcopy(module) for _ in range(count)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46nJzsFMKjl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, attention, source_attention, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.attention = attention\n",
        "        self.source_attention = source_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, inputs, memory, source_mask, target_mask):\n",
        "        inputs = self.sublayers[0](inputs, lambda inputs: self.attention(inputs, inputs, inputs, target_mask))\n",
        "        inputs  = self.sublayers[1](inputs, lambda inputs: self.source_attention(inputs, memory, memory, source_mask))\n",
        "        return self.sublayers[2](inputs, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfQrBlRNPb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    subsequent_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}